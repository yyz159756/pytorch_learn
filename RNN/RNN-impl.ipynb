{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[ 0.1128,  0.0155],\n",
      "        [-0.2187,  0.0653],\n",
      "        [ 0.2091,  0.0609]], requires_grad=True)\n",
      "weight_hh_l0 Parameter containing:\n",
      "tensor([[-0.3954, -0.2865, -0.1061],\n",
      "        [-0.1641,  0.5477, -0.1421],\n",
      "        [-0.1960,  0.5420, -0.3079]], requires_grad=True)\n",
      "bias_ih_l0 Parameter containing:\n",
      "tensor([ 0.1051,  0.3406, -0.1874], requires_grad=True)\n",
      "bias_hh_l0 Parameter containing:\n",
      "tensor([-0.3666,  0.2226,  0.5342], requires_grad=True)\n",
      "=========================\n",
      "tensor([[[-0.3690,  0.6068,  0.0786],\n",
      "         [-0.4361,  0.7997,  0.3363],\n",
      "         [-0.3997,  0.8359,  0.5713]],\n",
      "\n",
      "        [[-0.1562,  0.2542,  0.4664],\n",
      "         [-0.4288,  0.7636,  0.1443],\n",
      "         [-0.2091,  0.6744,  0.7675]]], grad_fn=<CopySlices>)\n",
      "tensor([[[-0.3690,  0.6068,  0.0786],\n",
      "         [-0.4361,  0.7997,  0.3363],\n",
      "         [-0.3997,  0.8359,  0.5713]],\n",
      "\n",
      "        [[-0.1562,  0.2542,  0.4664],\n",
      "         [-0.4288,  0.7636,  0.1443],\n",
      "         [-0.2091,  0.6744,  0.7675]]], grad_fn=<TransposeBackward1>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def rnn_impl(inp, weight_ih, weight_hh, bias_ih, bias_hh, h_prev):\n",
    "    \"\"\"\n",
    "    默认input是三维形状 [bs, len, in_size]\n",
    "    weight input hidden: [h_dim, input_size]\n",
    "    weight hidden hidden [h_dim, h_dim]\n",
    "    h_prev [bs, hidden_size]\n",
    "    output = [bs, T, h_dim]\n",
    "    \"\"\"\n",
    "    bs, seq_len, input_size = inp.shape\n",
    "    h_dim = weight_ih.shape[0]\n",
    "    # 初始化一个输出矩阵\n",
    "    h_out = torch.zeros(bs, seq_len, h_dim)\n",
    "\n",
    "    # RNN 复杂度和 序列长度 呈线性关系\n",
    "    for t in range(seq_len):\n",
    "        x = inp[:, t, :] # x shape = [bs, input_size]\n",
    "        x = x.unsqueeze(dim=2) # 扩充1维 [bs, input_size, 1]\n",
    "        # 对weight扩充，复制bs份 [bs, h_dim, input_size]\n",
    "        bw_ih = weight_ih.unsqueeze(dim=0).tile(bs, 1, 1)\n",
    "        bw_hh = weight_hh.unsqueeze(dim=0).tile(bs, 1, 1)\n",
    "        # [h_dim, input_size] @ [input_size, 1] = [h_dim, 1]\n",
    "        wih_times_x = torch.bmm(bw_ih, x).squeeze(-1) # [h_dim,]\n",
    "        whh_times_h = torch.bmm(bw_hh, h_prev.unsqueeze(2)).squeeze(-1) # [bs, h_dim]\n",
    "        h_prev = torch.tanh(wih_times_x + whh_times_h + bias_ih + bias_hh)\n",
    "        h_out[:, t, :] = h_prev\n",
    "\n",
    "    return h_out, h_prev.unsqueeze(0) # 因为官方输出hn是3维的\n",
    "\n",
    "\n",
    "def test_rnn_impl():\n",
    "    bs, seq_len = 2, 3\n",
    "    input_size, hidden_size = 2, 3\n",
    "    # 随机初始化一个输入\n",
    "    inp = torch.randn(bs, seq_len, input_size)\n",
    "    # 初始隐藏状态\n",
    "    h_prev = torch.zeros(bs, hidden_size)\n",
    "    rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "    res1, h_n1 = rnn(inp, h_prev.unsqueeze(dim=0))\n",
    "\n",
    "    # 取出RNN中的参数\n",
    "    for parameter, name in rnn.named_parameters():\n",
    "        print(parameter, name)\n",
    "    print(\"=========================\")\n",
    "\n",
    "    weight_ih = rnn.weight_ih_l0\n",
    "    weight_hh = rnn.weight_hh_l0\n",
    "    bias_ih = rnn.bias_ih_l0\n",
    "    bias_hh = rnn.bias_hh_l0\n",
    "    res2, h_n2 = rnn_impl(inp, weight_ih, weight_hh, bias_ih, bias_hh, h_prev)\n",
    "\n",
    "    print(res2)\n",
    "    print(res1)\n",
    "    print(torch.allclose(res1, res2))\n",
    "\n",
    "test_rnn_impl()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[-0.0896,  0.2071],\n",
      "        [ 0.1370,  0.1086],\n",
      "        [ 0.0022, -0.2868]], requires_grad=True)\n",
      "weight_hh_l0 Parameter containing:\n",
      "tensor([[ 0.2489,  0.5583,  0.0484],\n",
      "        [ 0.0950, -0.0650, -0.5075],\n",
      "        [-0.2816,  0.3662,  0.2736]], requires_grad=True)\n",
      "bias_ih_l0 Parameter containing:\n",
      "tensor([0.3195, 0.5765, 0.5038], requires_grad=True)\n",
      "bias_hh_l0 Parameter containing:\n",
      "tensor([ 0.0648, -0.1833, -0.1685], requires_grad=True)\n",
      "weight_ih_l0_reverse Parameter containing:\n",
      "tensor([[-0.4970, -0.4178],\n",
      "        [ 0.0177, -0.1048],\n",
      "        [ 0.5066,  0.3071]], requires_grad=True)\n",
      "weight_hh_l0_reverse Parameter containing:\n",
      "tensor([[ 0.4729,  0.2107,  0.0673],\n",
      "        [ 0.2862,  0.3941,  0.4727],\n",
      "        [-0.3802,  0.0082,  0.1022]], requires_grad=True)\n",
      "bias_ih_l0_reverse Parameter containing:\n",
      "tensor([-0.1800,  0.1912, -0.2034], requires_grad=True)\n",
      "bias_hh_l0_reverse Parameter containing:\n",
      "tensor([-0.4098, -0.2872, -0.2664], requires_grad=True)\n",
      "=========================\n",
      "tensor([[[ 0.4105,  0.4384,  0.2263, -0.8795, -0.5378,  0.0776],\n",
      "         [ 0.6632,  0.4136,  0.2761, -0.8197, -0.4786, -0.1017],\n",
      "         [ 0.6715,  0.0900,  0.4517,  0.1407, -0.0744, -0.8237]],\n",
      "\n",
      "        [[ 0.2833,  0.5993,  0.2295, -0.9724, -0.6355,  0.6544],\n",
      "         [ 0.8285,  0.2933,  0.1151, -0.6366, -0.5406, -0.5381],\n",
      "         [ 0.5441,  0.1991,  0.4808,  0.1321, -0.0029, -0.7964]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[ 0.4105,  0.4384,  0.2263, -0.8795, -0.5378,  0.0776],\n",
      "         [ 0.6632,  0.4136,  0.2761, -0.8197, -0.4786, -0.1017],\n",
      "         [ 0.6715,  0.0900,  0.4517,  0.1407, -0.0744, -0.8237]],\n",
      "\n",
      "        [[ 0.2833,  0.5993,  0.2295, -0.9724, -0.6355,  0.6544],\n",
      "         [ 0.8285,  0.2933,  0.1151, -0.6366, -0.5406, -0.5381],\n",
      "         [ 0.5441,  0.1991,  0.4808,  0.1321, -0.0029, -0.7964]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def bi_rnn_impl(inp, weight_ih, weight_hh, bias_ih, bias_hh, h_prev,\n",
    "                weight_ih_reverse, weight_hh_reverse, bias_ih_reverse,\n",
    "                bias_hh_reverse, h_prev_reverse):\n",
    "    bs, seq_len, input_size = inp.shape\n",
    "    h_dim = weight_ih.shape[0]\n",
    "    # 初始化一个输出矩阵,注意隐藏层维度要乘以2\n",
    "    h_out = torch.zeros(bs, seq_len, h_dim * 2)\n",
    "    # 调用两遍RNN\n",
    "    forward_output, _ = rnn_impl(inp, weight_ih, weight_hh,\n",
    "                              bias_ih, bias_hh, h_prev)\n",
    "    # flip 对input张量seq_len维度进行翻转\n",
    "    backward_output, _ = rnn_impl(torch.flip(inp, [1]), weight_ih_reverse,\n",
    "             weight_hh_reverse, bias_ih_reverse, bias_hh_reverse, h_prev_reverse)\n",
    "    # 将f和b output 填充到h_out中\n",
    "    h_out[:, :, :h_dim] = forward_output\n",
    "    # 注意要对backward_output进行seq翻转才能对应上seq序列\n",
    "    h_out[:, :, h_dim:] = torch.flip(backward_output, [1])\n",
    "    # 取最后一个时刻 T=-1 状态向量hn，注意hn shape=[D*layer, bs, h_dim]\n",
    "    hn = h_out[:, -1, :].reshape([bs, 2, h_dim]).transpose(0, 1)\n",
    "    return h_out, hn\n",
    "\n",
    "\n",
    "def test_bi_rnn_impl():\n",
    "    bs, seq_len = 2, 3\n",
    "    input_size, hidden_size = 2, 3\n",
    "    # 随机初始化一个输入\n",
    "    inp = torch.randn(bs, seq_len, input_size)\n",
    "    # 初始隐藏状态\n",
    "    h_prev = torch.zeros(2, bs, hidden_size)\n",
    "    bi_rnn = nn.RNN(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "    res1, h_n1 = bi_rnn(inp, h_prev)\n",
    "    # 取出RNN中的参数\n",
    "    for parameter, name in bi_rnn.named_parameters():\n",
    "        print(parameter, name)\n",
    "    print(\"=========================\")\n",
    "    weight_ih = bi_rnn.weight_ih_l0\n",
    "    weight_hh = bi_rnn.weight_hh_l0\n",
    "    bias_ih = bi_rnn.bias_ih_l0\n",
    "    bias_hh = bi_rnn.bias_hh_l0\n",
    "    weight_ih_reverse = bi_rnn.weight_ih_l0_reverse\n",
    "    weight_hh_reverse = bi_rnn.weight_hh_l0_reverse\n",
    "    bias_ih_reverse = bi_rnn.bias_ih_l0_reverse\n",
    "    bias_hh_reverse = bi_rnn.bias_hh_l0_reverse\n",
    "    res2, hn2 = bi_rnn_impl(inp,\n",
    "                            weight_ih,\n",
    "                            weight_hh,\n",
    "                            bias_ih,\n",
    "                            bias_hh,\n",
    "                            h_prev[0],\n",
    "                            weight_ih_reverse,\n",
    "                            weight_hh_reverse,\n",
    "                            bias_ih_reverse,\n",
    "                            bias_hh_reverse,\n",
    "                            h_prev[1])\n",
    "    print(res1)\n",
    "    print(res2)\n",
    "    print(torch.allclose(res1, res2))\n",
    "\n",
    "\n",
    "test_bi_rnn_impl()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}