{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 # # torch.Size([20, 4])\n",
      "weight_hh_l0 # # torch.Size([20, 5])\n",
      "bias_ih_l0 # # torch.Size([20])\n",
      "bias_hh_l0 # # torch.Size([20])\n",
      "++++++++++++++++++++++++++++++++++++++\n",
      "True\n",
      "tensor([[[-0.0253, -0.2863,  0.0269, -0.1507, -0.1089],\n",
      "         [ 0.0480, -0.0321,  0.0813, -0.1903, -0.1225],\n",
      "         [ 0.0992, -0.1596,  0.1424, -0.2368, -0.0450]],\n",
      "\n",
      "        [[-0.2811,  0.2649, -0.1801, -0.1610,  0.6522],\n",
      "         [-0.1713,  0.2637, -0.0560, -0.0387,  0.2390],\n",
      "         [-0.0578,  0.1238, -0.0354, -0.2534, -0.0732]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[-0.0253, -0.2863,  0.0269, -0.1507, -0.1089],\n",
      "         [ 0.0480, -0.0321,  0.0813, -0.1903, -0.1225],\n",
      "         [ 0.0992, -0.1596,  0.1424, -0.2368, -0.0450]],\n",
      "\n",
      "        [[-0.2811,  0.2649, -0.1801, -0.1610,  0.6522],\n",
      "         [-0.1713,  0.2637, -0.0560, -0.0387,  0.2390],\n",
      "         [-0.0578,  0.1238, -0.0354, -0.2534, -0.0732]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "def lstm_forward(inp, initial_states, w_ih, w_hh, b_ih, b_hh):\n",
    "    \"\"\"\n",
    "    input [bs, T, input_size]\n",
    "    \"\"\"\n",
    "    h0, c0 = initial_states\n",
    "    bs, seq_len, i_size = inp.shape\n",
    "    h_size = w_ih.shape[0] // 4 # w_ih [4 * h_dim, i_size]\n",
    "    bw_ih = w_ih.unsqueeze(0).tile(bs, 1, 1) # [bs, 4 * h_dim, i_size]\n",
    "    bw_hh = w_hh.unsqueeze(0).tile(bs, 1, 1) # [bs, 4 * h_dim, h_dim]\n",
    "    prev_h = h0 # [bs, h_d]\n",
    "    prev_c = c0\n",
    "    output_size = h_size\n",
    "    output = torch.randn(bs, seq_len, output_size)\n",
    "\n",
    "    # 对时间进行遍历\n",
    "    for t in range(seq_len):\n",
    "        x = inp[:, t, :] # [bs, input_size]\n",
    "        # 为了能进行bmm，对x增加一维 [bs, i_s, 1]\n",
    "        w_times_x = torch.bmm(bw_ih, x.unsqueeze(-1)).squeeze(-1) # [bs, 4h_d]\n",
    "        w_times_h_prev = torch.bmm(bw_hh, prev_h.unsqueeze(-1)).squeeze(-1) # [bs, 4h_d]\n",
    "        # 计算i门，取矩阵的前1/4\n",
    "        i = 0\n",
    "        i_t = torch.sigmoid(w_times_x[:, h_size*i:h_size*(1+i)] + w_times_h_prev[:,h_size*i:h_size*(1+i)] + b_ih[h_size*i:h_size*(1+i)] + b_hh[h_size*i:h_size*(1+i)])\n",
    "        # f门\n",
    "        i += 1\n",
    "        f_t = torch.sigmoid(w_times_x[:, h_size*i:h_size*(1+i)] + w_times_h_prev[:, h_size*i:h_size*(1+i)] + b_ih[h_size*i:h_size*(1+i)] + b_hh[h_size*i:h_size*(1+i)])\n",
    "        # g门\n",
    "        i += 1\n",
    "        g_t = torch.tanh(w_times_x[:, h_size*i:h_size*(1+i)] + w_times_h_prev[:, h_size*i:h_size*(1+i)] + b_ih[h_size*i:h_size*(1+i)] + b_hh[h_size*i:h_size*(1+i)])\n",
    "        # o门\n",
    "        i += 1\n",
    "        o_t = torch.sigmoid(w_times_x[:, h_size*i:h_size*(1+i)] + w_times_h_prev[:, h_size*i:h_size*(1+i)] + b_ih[h_size*i:h_size*(1+i)] + b_hh[h_size*i:h_size*(1+i)])\n",
    "\n",
    "        # cell\n",
    "        prev_c = f_t * prev_c + i_t * g_t\n",
    "\n",
    "        # h\n",
    "        prev_h = o_t * torch.tanh(prev_c)\n",
    "\n",
    "        output[:, t, :] = prev_h\n",
    "\n",
    "    return output, (prev_h, prev_c)\n",
    "\n",
    "\n",
    "def test_lstm_impl():\n",
    "    bs, t, i_size, h_size = 2, 3, 4, 5\n",
    "    inp = torch.randn(bs, t, i_size)\n",
    "    # 不需要训练\n",
    "    c0 = torch.randn(bs, h_size)\n",
    "    h0 = torch.randn(bs, h_size)\n",
    "\n",
    "    # 调用官方API\n",
    "    lstm_layer = nn.LSTM(i_size, h_size, batch_first=True)\n",
    "    output, _ = lstm_layer(inp, (h0.unsqueeze(0), c0.unsqueeze(0)))\n",
    "    for k, v in lstm_layer.named_parameters():\n",
    "        print(k, \"# #\", v.shape)\n",
    "\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "    w_ih = lstm_layer.weight_ih_l0\n",
    "    w_hh = lstm_layer.weight_hh_l0\n",
    "    b_ih = lstm_layer.bias_ih_l0\n",
    "    b_hh = lstm_layer.bias_hh_l0\n",
    "\n",
    "    output2, _ = lstm_forward(inp, (h0, c0), w_ih, w_hh, b_ih, b_hh)\n",
    "    print(torch.allclose(output2, output))\n",
    "    print(output)\n",
    "    print(output2)\n",
    "\n",
    "test_lstm_impl()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 # # torch.Size([20, 4])\n",
      "weight_hh_l0 # # torch.Size([20, 3])\n",
      "bias_ih_l0 # # torch.Size([20])\n",
      "bias_hh_l0 # # torch.Size([20])\n",
      "weight_hr_l0 # # torch.Size([3, 5])\n",
      "++++++++++++++++++++++++++++++++++++++\n",
      "True\n",
      "torch.Size([2, 3, 3])\n",
      "torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "def lstm_forward(inp, initial_states, w_ih, w_hh, b_ih, b_hh, w_hr=None):\n",
    "    \"\"\"\n",
    "    input [bs, T, input_size]\n",
    "    如果w_hr不是None说明是带projection\n",
    "    w_hr [p_dim, h_dim]\n",
    "    \"\"\"\n",
    "    h0, c0 = initial_states\n",
    "    bs, seq_len, i_size = inp.shape\n",
    "    h_size = w_ih.shape[0] // 4 # w_ih [4 * h_dim, i_size]\n",
    "    bw_ih = w_ih.unsqueeze(0).tile(bs, 1, 1) # [bs, 4 * h_dim, i_size]\n",
    "    bw_hh = w_hh.unsqueeze(0).tile(bs, 1, 1) # [bs, 4 * h_dim, h_dim]\n",
    "    prev_h = h0 # [bs, h_d]\n",
    "    prev_c = c0\n",
    "    if w_hr is not None:\n",
    "        output_size =  w_hr.shape[0]\n",
    "        bw_hr = w_hr.unsqueeze(0).tile(bs, 1, 1)\n",
    "    else:\n",
    "        output_size = h_size\n",
    "        bw_hr = None\n",
    "\n",
    "\n",
    "    output = torch.randn(bs, seq_len, output_size)\n",
    "\n",
    "    # 对时间进行遍历\n",
    "    for t in range(seq_len):\n",
    "        x = inp[:, t, :] # [bs, input_size]\n",
    "        # 为了能进行bmm，对x增加一维 [bs, i_s, 1]\n",
    "        w_times_x = torch.bmm(bw_ih, x.unsqueeze(-1)).squeeze(-1) # [bs, 4h_d]\n",
    "        w_times_h_prev = torch.bmm(bw_hh, prev_h.unsqueeze(-1)).squeeze(-1) # [bs, 4h_d]\n",
    "        # 计算i门，取矩阵的前1/4\n",
    "        i = 0\n",
    "        i_t = torch.sigmoid(w_times_x[:, h_size*i:h_size*(1+i)] + w_times_h_prev[:,h_size*i:h_size*(1+i)] + b_ih[h_size*i:h_size*(1+i)] + b_hh[h_size*i:h_size*(1+i)])\n",
    "        # f门\n",
    "        i += 1\n",
    "        f_t = torch.sigmoid(w_times_x[:, h_size*i:h_size*(1+i)] + w_times_h_prev[:, h_size*i:h_size*(1+i)] + b_ih[h_size*i:h_size*(1+i)] + b_hh[h_size*i:h_size*(1+i)])\n",
    "        # g门\n",
    "        i += 1\n",
    "        g_t = torch.tanh(w_times_x[:, h_size*i:h_size*(1+i)] + w_times_h_prev[:, h_size*i:h_size*(1+i)] + b_ih[h_size*i:h_size*(1+i)] + b_hh[h_size*i:h_size*(1+i)])\n",
    "        # o门\n",
    "        i += 1\n",
    "        o_t = torch.sigmoid(w_times_x[:, h_size*i:h_size*(1+i)] + w_times_h_prev[:, h_size*i:h_size*(1+i)] + b_ih[h_size*i:h_size*(1+i)] + b_hh[h_size*i:h_size*(1+i)])\n",
    "\n",
    "        # cell\n",
    "        prev_c = f_t * prev_c + i_t * g_t\n",
    "\n",
    "        # h\n",
    "        prev_h = o_t * torch.tanh(prev_c) # [bs, h_size]\n",
    "\n",
    "        # 对h进行压缩\n",
    "        if w_hr is not None:\n",
    "            prev_h = torch.bmm(bw_hr, prev_h.unsqueeze(-1)).squeeze(-1) # [bs, p_size]\n",
    "\n",
    "\n",
    "        output[:, t, :] = prev_h\n",
    "\n",
    "    return output, (prev_h, prev_c)\n",
    "\n",
    "def test_lstmp_impl():\n",
    "    bs, t, i_size, h_size = 2, 3, 4, 5\n",
    "    proj_size = 3\n",
    "    inp = torch.randn(bs, t, i_size)\n",
    "    # 不需要训练\n",
    "    c0 = torch.randn(bs, h_size)\n",
    "    h0 = torch.randn(bs, proj_size)\n",
    "\n",
    "    # 调用官方API\n",
    "    lstm_layer = nn.LSTM(i_size, h_size, batch_first=True, proj_size=proj_size)\n",
    "    output, _ = lstm_layer(inp, (h0.unsqueeze(0), c0.unsqueeze(0)))\n",
    "    for k, v in lstm_layer.named_parameters():\n",
    "        print(k, \"# #\", v.shape)\n",
    "\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "    w_ih = lstm_layer.weight_ih_l0\n",
    "    w_hh = lstm_layer.weight_hh_l0 # [bs, p_size] p_size相比h_d变小了\n",
    "    b_ih = lstm_layer.bias_ih_l0\n",
    "    b_hh = lstm_layer.bias_hh_l0\n",
    "    w_hr = lstm_layer.weight_hr_l0\n",
    "    output2, _ = lstm_forward(inp, (h0, c0), w_ih, w_hh, b_ih, b_hh, w_hr)\n",
    "    print(torch.allclose(output2, output))\n",
    "    print(output.shape)\n",
    "    print(output2.shape) # [bs, seq, p_size]\n",
    "\n",
    "test_lstmp_impl()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}