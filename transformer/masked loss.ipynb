{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.3811e-01, -5.3651e-02,  1.6222e+00, -3.7813e-01],\n",
      "         [-1.7034e+00,  2.9650e-01,  1.1526e-03, -2.5275e-01],\n",
      "         [-3.9760e-01, -1.6542e-01,  2.4655e-01,  1.2284e-01]],\n",
      "\n",
      "        [[-7.0630e-02,  8.6487e-01,  3.7207e-01,  5.2766e-01],\n",
      "         [-7.1508e-01, -6.3658e-01, -2.1381e-01, -4.3843e-02],\n",
      "         [-1.1693e+00,  2.1271e-02,  3.1061e-01, -9.8025e-01]]]) #logits\n",
      "tensor([[0, 0, 1],\n",
      "        [0, 0, 2]]) # label\n",
      "tensor(1.8467) # mean loss\n",
      "tensor([[2.1620, 2.8988, 1.5340],\n",
      "        [1.9351, 1.7386, 0.8116]]) # loss reduction=none\n"
     ]
    }
   ],
   "source": [
    "logits: torch.Tensor = torch.randn(2, 3, 4) # 假设batch_size=2，seq_len=3，单词表数=4\n",
    "print(logits, \"#logits\")\n",
    "# cross entropy 要求batch_size, C, d1...\n",
    "logits = logits.transpose(1,2)\n",
    "# 每一个样本（大小为2的batch）的每个位置上都有一个word在单词表中的index\n",
    "label = torch.randint(0, 4, (2,3)) # index ~ [0, C-1]\n",
    "print(label, \"# label\")\n",
    "# 计算交叉熵loss，每个句子的每个单词求一个交叉熵，所有单词加起来求一个平均， reduction='mean'\n",
    "loss = F.cross_entropy(logits, label)\n",
    "print(loss, \"# mean loss\")\n",
    "loss =  F.cross_entropy(logits, label, reduction='none')\n",
    "print(loss, \"# loss reduction=none\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3], dtype=torch.int32) # tgt_len\n",
      "tensor([[1., 1., 0.],\n",
      "        [1., 1., 1.]]) # mask\n",
      "tensor([[2.1620, 2.8988, 0.0000],\n",
      "        [1.9351, 1.7386, 0.8116]]) # mask loss\n"
     ]
    }
   ],
   "source": [
    "# 出现了padding情况，第一个句子长度是2，因此需要mask掉\n",
    "tgt_len = torch.Tensor([2,3]).to(torch.int32)\n",
    "print(tgt_len, \"# tgt_len\")\n",
    "mask = torch.cat([torch.unsqueeze(F.pad(torch.ones(L), (0, max(tgt_len) - L)), 0) for L in tgt_len],0)\n",
    "print(mask, \"# mask\")\n",
    "loss =  F.cross_entropy(logits, label, reduction='none') * mask\n",
    "print(loss, \"# mask loss\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    0, -100],\n",
      "        [   0,    0,    2]]) # with ignore_index label\n",
      "tensor([[2.1620, 2.8988, 0.0000],\n",
      "        [1.9351, 1.7386, 0.8116]]) # loss ignore_index，默认是-100，自动mask操作\n"
     ]
    }
   ],
   "source": [
    "# 使用ignore_index，默认是-100\n",
    "label[0, 2] = -100\n",
    "print(label, \"# with ignore_index label\")\n",
    "loss =  F.cross_entropy(logits, label, reduction='none')\n",
    "print(loss, \"# loss ignore_index，默认是-100，自动mask操作\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}