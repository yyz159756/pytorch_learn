{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 3])\n",
      "tensor([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 假设有两个句子\n",
    "batch_size = 2\n",
    "# 每个句子长度为2~5\n",
    "src_len = T.randint(2, 5, (batch_size, ))\n",
    "tgt_len = T.randint(2, 5, (batch_size, ))\n",
    "print(src_len)\n",
    "print(tgt_len)\n",
    "# 方便研究，我们写死\n",
    "src_len = T.Tensor([2, 4]).to(T.int32)\n",
    "tgt_len = T.Tensor([4, 3]).to(T.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 1]) # valid_encoder_pos size\n",
      "torch.Size([2, 4, 4]) # valid_encoder_pos_matrix size\n",
      "tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]]) # 4*4，valid_encoder_pos_matrix 第一行表示第一个单词对其他单词的有效性\n",
      "tensor([[[0., 0., 1., 1.],\n",
      "         [0., 0., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]]) # invalid_encoder_pos_matrix 0表示有效位置，1表示无效的位置\n",
      "tensor([[[False, False,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [ True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False]]]) # mask_encoder_self_attention True的地方需要mask\n"
     ]
    }
   ],
   "source": [
    "valid_encoder_pos = [torch.ones(L) for L in src_len]\n",
    "# padding至max句子长度\n",
    "valid_encoder_pos = list(map(lambda x: F.pad(x, (0, max(src_len) - len(x))), valid_encoder_pos))\n",
    "# 扩1维\n",
    "valid_encoder_pos = list(map(lambda x: T.unsqueeze(x, 0), valid_encoder_pos))\n",
    "# 拼接\n",
    "valid_encoder_pos = T.cat(valid_encoder_pos, 0)\n",
    "# 继续扩维 -> [2,4,1]\n",
    "valid_encoder_pos = T.unsqueeze(valid_encoder_pos, 2)\n",
    "print(valid_encoder_pos.shape, \"# valid_encoder_pos size\")\n",
    "# bmm：带批的矩阵相乘 [2,4,1] * [2,1,4]\n",
    "valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos, valid_encoder_pos.transpose(1, 2))\n",
    "print(valid_encoder_pos_matrix.shape, \"# valid_encoder_pos_matrix size\")\n",
    "print(valid_encoder_pos_matrix, \"# 4*4，valid_encoder_pos_matrix 第一行表示第一个单词对其他单词的有效性\")\n",
    "\n",
    "invalid_encoder_pos_matrix = 1-valid_encoder_pos_matrix # 取反\n",
    "print(invalid_encoder_pos_matrix, \"# invalid_encoder_pos_matrix 0表示有效位置，1表示无效的位置\")\n",
    "\n",
    "mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.bool)\n",
    "print(mask_encoder_self_attention, \"# mask_encoder_self_attention True的地方需要mask\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4961, 0.5039, 0.0000, 0.0000],\n",
      "         [0.9147, 0.0853, 0.0000, 0.0000],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500]],\n",
      "\n",
      "        [[0.2296, 0.1781, 0.2676, 0.3246],\n",
      "         [0.5732, 0.1509, 0.0409, 0.2349],\n",
      "         [0.1378, 0.3817, 0.0573, 0.4232],\n",
      "         [0.0592, 0.6797, 0.0724, 0.1887]]]) # 注意力权重\n"
     ]
    }
   ],
   "source": [
    "# 用法，随机生成一个score\n",
    "score = torch.randn(batch_size,max(src_len), max(src_len))\n",
    "masked_score = score.masked_fill(mask_encoder_self_attention, -1e9) # 传入一个布尔型的张量，mask的地方置为负无穷\n",
    "# 再对masked的score计算一个softmax, 计算出注意力的权重\n",
    "prob = F.softmax(masked_score, -1)\n",
    "print(prob, \"# 注意力权重\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [0.]]]) # valid_decoder_pos\n",
      "torch.Size([2, 4, 1]) # valid_decoder_pos size\n",
      "tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 0., 0.],\n",
      "         [1., 1., 0., 0.],\n",
      "         [1., 1., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0.]]]) # valid_cross_pos 目标序列对源序列关联有效矩阵，1表示有效, src len:[2,4] tgt len:[4,3], 所以一个tensor表示tgt 4个单词注意力到src的2个单词\n",
      "tensor([[[0., 0., 1., 1.],\n",
      "         [0., 0., 1., 1.],\n",
      "         [0., 0., 1., 1.],\n",
      "         [0., 0., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [1., 1., 1., 1.]]]) # invalid_cross_pos_matrix，1表示无效\n",
      "tensor([[[False, False,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [False, False,  True,  True]],\n",
      "\n",
      "        [[False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [ True,  True,  True,  True]]]) # cross_attention_mask， True表示需要mask的地方\n"
     ]
    }
   ],
   "source": [
    "# intra-attention mask实现\n",
    "# 公式 Q valid pos @ K^T valid pos, shape:[batch_size, tgt_seq_len, src_seq_le]\n",
    "# 构造tgt mask\n",
    "valid_decoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), (0, max(tgt_len)-L)), 0)for L in tgt_len]), 2)\n",
    "print(valid_decoder_pos, \"# valid_decoder_pos\")\n",
    "print(valid_decoder_pos.shape, \"# valid_decoder_pos size\")\n",
    "# 构造出交叉有效位置 -- mask矩阵\n",
    "valid_cross_pos_matrix = torch.bmm(valid_decoder_pos, valid_encoder_pos.transpose(1, 2))\n",
    "print(valid_cross_pos_matrix, \"# valid_cross_pos 目标序列对源序列关联有效矩阵，1表示有效, src len:[2,4] tgt len:[4,3], 所以一个tensor表示tgt 4个单词注意力到src的2个单词\")\n",
    "# 下面其实不用置反操作也行，使用masked_fill时候，参数mask == 0\n",
    "invalid_cross_pos_matrix = 1-valid_cross_pos_matrix\n",
    "print(invalid_cross_pos_matrix, \"# invalid_cross_pos_matrix，1表示无效\")\n",
    "# 转换为bool类型\n",
    "cross_attention_mask = invalid_cross_pos_matrix.to(torch.bool)\n",
    "print(cross_attention_mask, \"# cross_attention_mask， True表示需要mask的地方\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1.]]), tensor([[1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0.]])] # valid_decoder_tri_matrix， 第二张量也变成四行了\n",
      "torch.Size([2, 4, 4]) # valid_decoder_tri_matrix,扩维 拼接\n",
      "其实已经构建好了decoder self attention mask了\n",
      "tensor([[[False,  True,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [False, False, False,  True],\n",
      "         [False, False, False, False]],\n",
      "\n",
      "        [[False,  True,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [False, False, False,  True],\n",
      "         [ True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "# 构造decoder self attention mask\n",
    "# 构建一个下三角矩阵\n",
    "valid_decoder_tri_matrix = [torch.tril(torch.ones(L,L)) for L in tgt_len] # tgt_len:[4, 3]\n",
    "# pad操作是从低维到高维的，向后填充和向下填充\n",
    "valid_decoder_tri_matrix = list(map(lambda x: F.pad(x, (0,max(tgt_len) - len(x),0,max(tgt_len) - len(x))), valid_decoder_tri_matrix))\n",
    "print(valid_decoder_tri_matrix, \"# valid_decoder_tri_matrix， 第二张量也变成四行了\")\n",
    "valid_decoder_tri_matrix = list(map(lambda x: torch.unsqueeze(x, 0), valid_decoder_tri_matrix))\n",
    "valid_decoder_tri_matrix = torch.cat(valid_decoder_tri_matrix)\n",
    "print(valid_decoder_tri_matrix.shape, \"# valid_decoder_tri_matrix,扩维 拼接\")\n",
    "print(\"已经构建好了decoder self attention mask了\")\n",
    "invalid_decoder_tri_matrix = 1-valid_decoder_tri_matrix\n",
    "invalid_decoder_tri_matrix = invalid_decoder_tri_matrix.to(torch.bool)\n",
    "print(invalid_decoder_tri_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.8276e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [ 2.8688e-01,  5.9017e-01, -1.0000e+09, -1.0000e+09],\n",
      "         [ 8.5252e-01, -2.1590e+00, -6.6107e-01, -1.0000e+09],\n",
      "         [-1.4719e+00, -6.4130e-01, -1.3947e+00, -1.7222e+00]],\n",
      "\n",
      "        [[-3.9795e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-5.3399e-01,  1.4487e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0009e+00,  2.0542e+00,  7.6253e-01, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]]])\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4248, 0.5752, 0.0000, 0.0000],\n",
      "         [0.7878, 0.0388, 0.1734, 0.0000],\n",
      "         [0.1940, 0.4453, 0.2096, 0.1511]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1210, 0.8790, 0.0000, 0.0000],\n",
      "         [0.0356, 0.7565, 0.2079, 0.0000],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500]]])\n"
     ]
    }
   ],
   "source": [
    "# 测试decoder self attention mask\n",
    "score: torch.Tensor = torch.randn(batch_size, max(tgt_len), max(tgt_len))\n",
    "score.masked_fill_(invalid_decoder_tri_matrix, -1e9)\n",
    "print(score)\n",
    "prob = F.softmax(score, dim=-1)\n",
    "print(prob)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}