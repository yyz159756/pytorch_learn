{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6998, -1.8585,  1.6903,  0.3141, -0.3566,  0.8918,  0.4699],\n",
      "        [ 1.3772, -0.1361, -1.2222,  0.1592, -0.0675, -0.9032, -0.8027],\n",
      "        [ 0.8802, -2.5107, -0.8699, -1.6091,  0.4236, -1.6767, -0.7457],\n",
      "        [ 0.1699,  0.3757, -0.2792,  0.3602, -0.1941,  0.8076, -0.6239],\n",
      "        [ 0.9251, -0.7313, -1.3957, -0.2472, -0.7104,  0.5917, -0.0107],\n",
      "        [-1.5601,  0.6924,  0.3242, -3.0884,  0.8980, -1.0992,  1.1466],\n",
      "        [ 0.2344,  0.7881,  0.7016,  0.1453, -0.9354,  1.3735, -0.5850]])\n",
      "tensor([[ 0.6998, -1.8585,  1.6903],\n",
      "        [ 1.3772, -0.1361, -1.2222],\n",
      "        [ 0.8802, -2.5107, -0.8699]])\n",
      "tensor([[ 0.6998,  1.6903, -0.3566],\n",
      "        [ 0.8802, -0.8699,  0.4236],\n",
      "        [ 0.9251, -1.3957, -0.7104]])\n",
      "tensor([[ 0.6998,  0.3141,  0.4699],\n",
      "        [ 0.1699,  0.3602, -0.6239],\n",
      "        [ 0.2344,  0.1453, -0.5850]])\n"
     ]
    }
   ],
   "source": [
    "def dilation_demo():\n",
    "    a = torch.randn(7,7)\n",
    "    print(a)\n",
    "    # dilation = 1\n",
    "    print(a[0:3, 0:3])\n",
    "    # dilation = 2\n",
    "    print(a[0:5:2, 0:5:2])\n",
    "    # dilation = 3\n",
    "    dilation = 3\n",
    "    print(a[0:7:3, 0:7:3])\n",
    "\n",
    "dilation_demo()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def matrix_multiplication_for_conv2d_final(input, kernel, bias=None, stride=1,\n",
    "                                           padding=0, dilation=1, groups=1):\n",
    "    if padding > 0:\n",
    "        input = F.pad(input, (padding, padding, padding, padding, 0,0,0,0))\n",
    "\n",
    "    # batch_size, in_channel, input h, input w\n",
    "    bs, ic, ih, iw = input.shape\n",
    "    # out_channel, _, kernel h, kernel w\n",
    "    oc, _ic, kh, kw = kernel.shape\n",
    "    if bias is None:\n",
    "        bias = torch.zeros(oc)\n",
    "    # 考虑groups情况, 确保ic,oc能被groups整除\n",
    "    assert oc % groups == 0 and ic % groups == 0, \"groups必须同时被通道数整除！\"\n",
    "    # reshape一下，把groups拆开\n",
    "    input = input.reshape((bs, groups, ic//groups, ih, iw))\n",
    "    kernel = kernel.reshape((groups, oc//groups, ic//groups, kh, kw))\n",
    "    # 相邻点之间插入dilation-1个空洞，插入kh - 1次，所以增加的距离一共是\n",
    "    kh = (dilation - 1) * (kh - 1) + kh\n",
    "    kw = (dilation - 1) * (kw - 1) + kw\n",
    "    # 输出 高度和宽度, 不需要考虑 dilation了，因为已经在kh和kw里面了\n",
    "    oh = int(math.floor((ih - kh)/stride)) + 1\n",
    "    ow = int(math.floor((iw - kw)/stride)) + 1\n",
    "    output_shape = (bs, groups, oc//groups, oh, ow)\n",
    "    # 初始化输出\n",
    "    output = torch.zeros(output_shape)\n",
    "    # 遍历计算\n",
    "    for ind in  range(bs): # batch遍历\n",
    "        for g in range(groups): # 群组遍历\n",
    "            for oc_ind in range(oc//groups): # 对分组的输出通道遍历\n",
    "                for ic_ind in range(ic//groups): # 对分组的输入通道遍历\n",
    "                    for i in range(0, ih-kh+1, stride): # 高度\n",
    "                        for j in range(0, iw-kw+1, stride): # 宽度\n",
    "                            # 取出区域\n",
    "                            region = input[ind, g, ic_ind, i:i+kh:dilation,         j:j+kw:dilation]\n",
    "                            output[ind, g, oc_ind, int(i/stride), int(j/stride)] += torch.sum(region * kernel[g, oc_ind, ic_ind])\n",
    "                # bias偏置，计算走过多少个通道\n",
    "                output[ind, g, oc_ind] += bias[g*(oc//groups) + oc_ind]\n",
    "    # 还原回4维\n",
    "    output = output.reshape((bs, oc, oh, ow))\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def test_conv2d_final():\n",
    "    bs, ic, ih, iw = 2, 2, 5, 5\n",
    "    kh, kw = 3, 3\n",
    "    oc = 4\n",
    "    groups, dilation, stride = 2, 2, 2\n",
    "    padding = 1\n",
    "\n",
    "\n",
    "    input = torch.randn(bs, ic, ih, iw)\n",
    "    # groups大于1，kernel数量会减小，输入通道数减小\n",
    "    kernel = torch.randn(oc, ic//groups, kh, kw)\n",
    "    bias = torch.randn(oc)\n",
    "\n",
    "    py_res = F.conv2d(input, kernel, bias=bias, padding=padding, stride=stride,\n",
    "                    dilation=dilation, groups=groups)\n",
    "\n",
    "    my_res = matrix_multiplication_for_conv2d_final(\n",
    "        input, kernel, bias=bias,padding=padding, stride=stride,\n",
    "        dilation=dilation, groups=groups)\n",
    "\n",
    "    flag = torch.allclose(py_res, my_res)\n",
    "    print(flag)\n",
    "\n",
    "test_conv2d_final()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}