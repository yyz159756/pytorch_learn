{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.4749,  0.2363,  0.6269,  0.9417],\n",
      "          [-0.5951, -1.9033, -1.2701,  0.2512],\n",
      "          [ 0.5078,  0.7907,  0.0940,  1.1174],\n",
      "          [ 0.1305, -0.7959, -0.6058,  0.8090]]]]) # conv_input\n",
      "tensor([[[[-0.3745, -0.7408],\n",
      "          [-0.2931, -0.7413]]]], grad_fn=<ConvolutionBackward0>) # conv_output\n",
      "torch.Size([1, 1, 3, 3]) # [out_ch, in_ch, h, w]\n"
     ]
    }
   ],
   "source": [
    "# 实例化一个二维卷积层\n",
    "in_channel = 1\n",
    "out_channel = 1\n",
    "kernel_size = 3\n",
    "bias_flag = False\n",
    "height = 4\n",
    "weight = 4\n",
    "batch_size = 1\n",
    "input_size1 = [batch_size, in_channel, height, weight] # [1,4,4]\n",
    "\n",
    "conv_layer = torch.nn.Conv2d(in_channel, out_channel, kernel_size, bias=bias_flag)\n",
    "conv_input = torch.randn(input_size1)\n",
    "print(conv_input, \"# conv_input\")\n",
    "conv_output = conv_layer(conv_input)\n",
    "print(conv_output, \"# conv_output\")\n",
    "print(conv_layer.weight.shape, \"# [out_ch, in_ch, h, w]\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.3745, -0.7408],\n",
      "          [-0.2931, -0.7413]]]], grad_fn=<ConvolutionBackward0>) # conv_F_output\n"
     ]
    }
   ],
   "source": [
    "# 使用functional api\n",
    "conv_F_output = F.conv2d(conv_input, conv_layer.weight)\n",
    "print(conv_F_output, \"# conv_F_output\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0155, 2.0155, 2.0155, 2.0155, 1.0155],\n",
      "        [2.0155, 3.5155, 3.5155, 3.5155, 2.0155],\n",
      "        [2.0155, 3.5155, 3.5155, 3.5155, 2.0155],\n",
      "        [2.0155, 3.5155, 3.5155, 3.5155, 2.0155],\n",
      "        [1.0155, 2.0155, 2.0155, 2.0155, 1.0155]])\n",
      "tensor([[[[1.0155, 2.0155, 2.0155, 2.0155, 1.0155],\n",
      "          [2.0155, 3.5155, 3.5155, 3.5155, 2.0155],\n",
      "          [2.0155, 3.5155, 3.5155, 3.5155, 2.0155],\n",
      "          [2.0155, 3.5155, 3.5155, 3.5155, 2.0155],\n",
      "          [1.0155, 2.0155, 2.0155, 2.0155, 1.0155]]]])\n"
     ]
    }
   ],
   "source": [
    "# 原始的矩阵运算实现二位卷积\n",
    "tmp_input = torch.ones(5, 5)\n",
    "tmp_kernel = torch.ones(3, 3) / 2\n",
    "tmp_bias = torch.randn(1)\n",
    "def matrix_multiplication_for_conv2d(input_tensor: torch.Tensor,\n",
    "                                     kernel: torch.Tensor, stride=1,\n",
    "                                     padding=0, bias=0):\n",
    "    \"\"\"\n",
    "    不考虑batch_size维度和channel维度\n",
    "    \"\"\"\n",
    "    if padding > 0:\n",
    "        input_tensor = F.pad(input_tensor, (padding, padding, padding, padding))\n",
    "    input_h, input_w = input_tensor.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    output_h = math.floor((input_h - kernel_h)/stride) + 1\n",
    "    output_w = math.floor((input_w - kernel_w)/stride) + 1\n",
    "    # 初始化一个输出矩阵\n",
    "    output = torch.zeros(output_h, output_w)\n",
    "    # 遍历\n",
    "    for i in range(0, input_h - kernel_h + 1, stride):\n",
    "        for j in range(0, input_w - kernel_w + 1, stride):\n",
    "            # 取输入的区域\n",
    "            region = input_tensor[i:i+kernel_h, j:j+kernel_w]\n",
    "            # 区域与kernel点乘，逐元素相乘\n",
    "            output[int(i/stride), int(j/stride)] =  torch.sum(region * kernel) + bias\n",
    "    return output\n",
    "\n",
    "\n",
    "res1 = matrix_multiplication_for_conv2d(tmp_input, tmp_kernel, padding=1,\n",
    "                                        bias=tmp_bias)\n",
    "\n",
    "res2 = F.conv2d(tmp_input.reshape(1,1,tmp_input.shape[0], tmp_input.shape[1]),\n",
    "                tmp_kernel.reshape(1,1,tmp_kernel.shape[0], tmp_kernel.shape[1]),\n",
    "                padding=1, bias=tmp_bias)\n",
    "print(res1)\n",
    "print(res2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[2.7592, 3.7592, 3.7592, 3.7592, 2.7592],\n",
      "        [3.7592, 5.2592, 5.2592, 5.2592, 3.7592],\n",
      "        [3.7592, 5.2592, 5.2592, 5.2592, 3.7592],\n",
      "        [3.7592, 5.2592, 5.2592, 5.2592, 3.7592],\n",
      "        [2.7592, 3.7592, 3.7592, 3.7592, 2.7592]])\n",
      "tensor([[[[2.7592, 3.7592, 3.7592, 3.7592, 2.7592],\n",
      "          [3.7592, 5.2592, 5.2592, 5.2592, 3.7592],\n",
      "          [3.7592, 5.2592, 5.2592, 5.2592, 3.7592],\n",
      "          [3.7592, 5.2592, 5.2592, 5.2592, 3.7592],\n",
      "          [2.7592, 3.7592, 3.7592, 3.7592, 2.7592]]]])\n"
     ]
    }
   ],
   "source": [
    "# 原始的矩阵运算实现二位卷积\n",
    "tmp_input = torch.ones(5, 5)\n",
    "tmp_kernel = torch.ones(3, 3) / 2\n",
    "tmp_bias = torch.randn(1)\n",
    "def matrix_multiplication_for_conv2d_flatten(input_tensor: torch.Tensor,\n",
    "                                     kernel: torch.Tensor, stride=1,\n",
    "                                     padding=0, bias=0):\n",
    "    \"\"\"\n",
    "    不考虑batch_size维度和channel维度，flatten版本\n",
    "    \"\"\"\n",
    "    if padding > 0:\n",
    "        input_tensor = F.pad(input_tensor, (padding, padding, padding, padding))\n",
    "    input_h, input_w = input_tensor.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    output_h = math.floor((input_h - kernel_h)/stride) + 1\n",
    "    output_w = math.floor((input_w - kernel_w)/stride) + 1\n",
    "    # 初始化一个输出矩阵\n",
    "    output = torch.zeros(output_h, output_w)\n",
    "    # 存储拉平后的特征区域\n",
    "    region_matrix = torch.zeros(output.numel(), kernel.numel())\n",
    "    # 将kernel转成列向量\n",
    "    kernel_matrix = kernel.reshape((kernel.numel(), 1))\n",
    "    row_index = 0\n",
    "    # 遍历\n",
    "    for i in range(0, input_h - kernel_h + 1, stride):\n",
    "        for j in range(0, input_w - kernel_w + 1, stride):\n",
    "            # 取输入的区域\n",
    "            region = input_tensor[i:i+kernel_h, j:j+kernel_w]\n",
    "            region_vector = torch.flatten(region)\n",
    "            region_matrix[row_index] = region_vector\n",
    "            row_index += 1\n",
    "    # 矩阵乘法\n",
    "    output_matrix = region_matrix @ kernel_matrix\n",
    "    output = output_matrix.reshape((output_h, output_w)) + bias\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "res1 = matrix_multiplication_for_conv2d_flatten(tmp_input, tmp_kernel, padding=1,\n",
    "                                        bias=tmp_bias)\n",
    "\n",
    "res2 = F.conv2d(tmp_input.reshape(1,1,tmp_input.shape[0], tmp_input.shape[1]),\n",
    "                tmp_kernel.reshape(1,1,tmp_kernel.shape[0], tmp_kernel.shape[1]),\n",
    "                padding=1, bias=tmp_bias)\n",
    "flag = torch.allclose(res1, res2)\n",
    "print(flag)\n",
    "print(res1)\n",
    "print(res2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[[[3.0971, 5.0971, 3.0971],\n",
      "          [5.0971, 8.0971, 5.0971],\n",
      "          [3.0971, 5.0971, 3.0971]],\n",
      "\n",
      "         [[4.0109, 6.0109, 4.0109],\n",
      "          [6.0109, 9.0109, 6.0109],\n",
      "          [4.0109, 6.0109, 4.0109]],\n",
      "\n",
      "         [[2.4993, 4.4993, 2.4993],\n",
      "          [4.4993, 7.4993, 4.4993],\n",
      "          [2.4993, 4.4993, 2.4993]]],\n",
      "\n",
      "\n",
      "        [[[3.0971, 5.0971, 3.0971],\n",
      "          [5.0971, 8.0971, 5.0971],\n",
      "          [3.0971, 5.0971, 3.0971]],\n",
      "\n",
      "         [[4.0109, 6.0109, 4.0109],\n",
      "          [6.0109, 9.0109, 6.0109],\n",
      "          [4.0109, 6.0109, 4.0109]],\n",
      "\n",
      "         [[2.4993, 4.4993, 2.4993],\n",
      "          [4.4993, 7.4993, 4.4993],\n",
      "          [2.4993, 4.4993, 2.4993]]]])\n",
      "tensor([[[[3.0971, 5.0971, 3.0971],\n",
      "          [5.0971, 8.0971, 5.0971],\n",
      "          [3.0971, 5.0971, 3.0971]],\n",
      "\n",
      "         [[4.0109, 6.0109, 4.0109],\n",
      "          [6.0109, 9.0109, 6.0109],\n",
      "          [4.0109, 6.0109, 4.0109]],\n",
      "\n",
      "         [[2.4993, 4.4993, 2.4993],\n",
      "          [4.4993, 7.4993, 4.4993],\n",
      "          [2.4993, 4.4993, 2.4993]]],\n",
      "\n",
      "\n",
      "        [[[3.0971, 5.0971, 3.0971],\n",
      "          [5.0971, 8.0971, 5.0971],\n",
      "          [3.0971, 5.0971, 3.0971]],\n",
      "\n",
      "         [[4.0109, 6.0109, 4.0109],\n",
      "          [6.0109, 9.0109, 6.0109],\n",
      "          [4.0109, 6.0109, 4.0109]],\n",
      "\n",
      "         [[2.4993, 4.4993, 2.4993],\n",
      "          [4.4993, 7.4993, 4.4993],\n",
      "          [2.4993, 4.4993, 2.4993]]]])\n"
     ]
    }
   ],
   "source": [
    "# 原始的矩阵运算实现二位卷积\n",
    "batch_size = 2\n",
    "in_channel = 2\n",
    "inh = 5\n",
    "inw = 5\n",
    "out_channel = 3\n",
    "kh = 3\n",
    "kw = 3\n",
    "tmp_input = torch.ones(batch_size, in_channel, inh, inw)\n",
    "tmp_kernel = torch.ones(out_channel, in_channel, kh, kw) / 2\n",
    "# bias的是个标量，但是对应每个输出通道不同\n",
    "tmp_bias = torch.randn(out_channel)\n",
    "def matrix_multiplication_for_conv2d_full(input_tensor: torch.Tensor,\n",
    "                                     kernel: torch.Tensor, stride=1,\n",
    "                                     padding=0, bias=0):\n",
    "    \"\"\"\n",
    "    考虑batch_size维度和channel维度\n",
    "    input和kernel都是4维，\n",
    "    \"\"\"\n",
    "    if padding > 0:\n",
    "        # batch_size，channel维度都不需要填充\n",
    "        input_tensor = F.pad(input_tensor, (padding, padding, padding, padding,0,0,0,0))\n",
    "    if bias is None:\n",
    "        bias = torch.zeros(out_channel)\n",
    "    # input shape: batch_size, in channel, h, w\n",
    "    bs, in_ch, input_h, input_w = input_tensor.shape\n",
    "    # kernel shape: out_channel, in_channel, kernel_h, kernel_w\n",
    "    out_ch, in_ch, kernel_h, kernel_w = kernel.shape\n",
    "\n",
    "    output_h = math.floor((input_h - kernel_h)/stride) + 1\n",
    "    output_w = math.floor((input_w - kernel_w)/stride) + 1\n",
    "    # 初始化一个输出矩阵\n",
    "    output = torch.zeros(bs, out_ch, output_h, output_w)\n",
    "    # 5层遍历,逐层遍历batch_size, out_channel, in_channel, h, w\n",
    "    for ind in range(bs):\n",
    "        for oc in range(out_ch):\n",
    "            for ic in range(in_ch):\n",
    "                for i in range(0, input_h - kernel_h + 1, stride):\n",
    "                    for j in range(0, input_w - kernel_w + 1, stride):\n",
    "                        # 取输入的区域\n",
    "                        region = input_tensor[ind, ic, i:i+kernel_h, j:j+kernel_w]\n",
    "                        # 输入区域与kernel计算卷积，逐元素相乘,输出通道是所有输入通道的求和，是+=\n",
    "                        # kernel也要取出区域，取第oc通道的第ic通道\n",
    "                        output[ind, oc, int(i/stride), int(j/stride)] \\\n",
    "                            +=  torch.sum(region * kernel[oc, ic])\n",
    "            # bias是对每个output channel相加\n",
    "            output[ind, oc] += bias[oc]\n",
    "    return output\n",
    "\n",
    "\n",
    "res1 = matrix_multiplication_for_conv2d_full(tmp_input, tmp_kernel, padding=1,\n",
    "                                        bias=tmp_bias, stride=2)\n",
    "\n",
    "res2 = F.conv2d(tmp_input, tmp_kernel,\n",
    "                padding=1, bias=tmp_bias, stride=2)\n",
    "flag = torch.allclose(res1, res2)\n",
    "print(flag)\n",
    "print(res1)\n",
    "print(res2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 30, 4]) # ouput size\n"
     ]
    }
   ],
   "source": [
    "# Unfold用法\n",
    "kernel_h = 2\n",
    "kernel_w = 3\n",
    "in_channel = 5\n",
    "in_h = 3\n",
    "in_w = 4\n",
    "batch_size = 2\n",
    "out_channel = 1\n",
    "unfold = nn.Unfold(kernel_size=(kernel_h, kernel_w))\n",
    "input = torch.randn(batch_size, in_channel, in_h, in_w)\n",
    "output = unfold(input)\n",
    "print(output.size(), \"# ouput size\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "# kernel展开\n",
    "def get_kernel_matrix(kernel, input_size):\n",
    "    \"\"\"先不考虑batch，channel，padding. 并假设stride=1\n",
    "    得到kernel矩阵，将kernel拉长、填充\n",
    "    如3×3的kernel变成5×5的向量\n",
    "    将所有的向量堆叠起来成一个矩阵\n",
    "    \"\"\"\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    input_h, input_w = input_size.shape\n",
    "    num_out_feat_map = (input_h - kernel_h + 1) * (input_w - kernel_w + 1)\n",
    "    # 初始化res矩阵\n",
    "    result = torch.zeros((num_out_feat_map, input_h*input_w))\n",
    "    # 分别对高度维和宽度维循环\n",
    "    count = 0\n",
    "    for i in range(0, input_h - kernel_h + 1, 1):\n",
    "        for j in range(0, input_w - kernel_w + 1, 1):\n",
    "            # pad操作：先左右后上下填充0, 使得填充后的大小和输入的大小一致\n",
    "            padded_kernel = F.pad(kernel,[j, input_w - kernel_w - j, i, input_h - kernel_h - i])\n",
    "            # 每次填充后的kernel拉直送入result中\n",
    "            result[count] = padded_kernel.flatten()\n",
    "            count += 1\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2622],\n",
      "        [ 5.0887],\n",
      "        [-0.3410],\n",
      "        [ 3.7604]]) \n",
      " tensor([[[[-1.2622,  5.0887],\n",
      "          [-0.3410,  3.7604]]]])\n"
     ]
    }
   ],
   "source": [
    "def test_get_kernel_matrix():\n",
    "    kernel = torch.randn(3, 3)\n",
    "    input = torch.randn(4, 4)\n",
    "    kernel_matrix = get_kernel_matrix(kernel, input)\n",
    "    # print(kernel_matrix, '\\n', kernel_matrix.shape)\n",
    "    mm_conv2d_output = kernel_matrix @ input.reshape((-1, 1))\n",
    "    pytorch_conv2d_output = F.conv2d(input.unsqueeze(0).unsqueeze(0),\n",
    "                                     kernel.unsqueeze(0).unsqueeze(0))\n",
    "    print(mm_conv2d_output, \"\\n\", pytorch_conv2d_output)\n",
    "\n",
    "test_get_kernel_matrix()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_2216\\3567069564.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 17\u001B[1;33m \u001B[0mtest_transpose_conv2d_demo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_2216\\3567069564.py\u001B[0m in \u001B[0;36mtest_transpose_conv2d_demo\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mtest_transpose_conv2d_demo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m     \u001B[1;31m# 转置卷积实现上采样\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m     \u001B[0mkernel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m3\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m     \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m4\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mkernel_matrix\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_kernel_matrix\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkernel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def test_transpose_conv2d_demo():\n",
    "    # 转置卷积实现上采样\n",
    "    kernel = torch.randn(3, 3)\n",
    "    input = torch.randn(4, 4)\n",
    "    kernel_matrix = get_kernel_matrix(kernel, input)\n",
    "    mm_conv2d_output = kernel_matrix @ input.reshape((-1, 1))\n",
    "    pytorch_conv2d_output = F.conv2d(input.unsqueeze(0).unsqueeze(0),\n",
    "                                     kernel.unsqueeze(0).unsqueeze(0))\n",
    "    # [16,4] @ [4,1]\n",
    "    mm_transposed_conv2d_output = kernel_matrix.transpose(-1, -2) @ mm_conv2d_output\n",
    "    py_transpose_conv2d_output = F.conv_transpose2d(pytorch_conv2d_output,\n",
    "                                                    kernel.unsqueeze(0).unsqueeze(0))\n",
    "    print(mm_transposed_conv2d_output.reshape(4,4))\n",
    "    print(py_transpose_conv2d_output)\n",
    "\n",
    "\n",
    "test_transpose_conv2d_demo()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}